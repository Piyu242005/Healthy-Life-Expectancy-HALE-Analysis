





# Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# Set plotting style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

print("‚úÖ Libraries imported successfully!")


# Load the dataset
df = pd.read_csv('C64284D_ALL_LATEST.csv')

# Display basic information about the dataset
print("=" * 60)
print("DATASET OVERVIEW")
print("=" * 60)
print(f"\nüìä Dataset Shape: {df.shape[0]} rows √ó {df.shape[1]} columns")
print(f"\nüìã Column Names:\n{df.columns.tolist()}")

# Display first few rows
print("\n" + "=" * 60)
print("FIRST 5 ROWS")
print("=" * 60)
df.head()


# Data Types and Info
print("=" * 60)
print("DATA TYPES & MEMORY USAGE")
print("=" * 60)
df.info()





# Select relevant columns for analysis
columns_to_keep = [
    'DIM_TIME',           # Year
    'GEO_NAME_SHORT',     # Country Name
    'DIM_SEX',            # Sex (Male/Female/Both Sexes)
    'AMOUNT_N',           # Healthy Life Expectancy (HALE) value
    'AMOUNT_NL',          # Lower bound (confidence interval)
    'AMOUNT_NU'           # Upper bound (confidence interval)
]

# Create a clean dataframe with selected columns
df_clean = df[columns_to_keep].copy()

# Rename columns for better readability
df_clean.columns = [
    'Year',
    'Country',
    'Sex',
    'HALE',           # Healthy Life Expectancy
    'HALE_Lower',     # Lower confidence bound
    'HALE_Upper'      # Upper confidence bound
]

print("‚úÖ Columns selected and renamed successfully!")
print(f"\nüìä Clean Dataset Shape: {df_clean.shape}")
df_clean.head(10)





# Check for missing values
print("=" * 60)
print("MISSING VALUES ANALYSIS")
print("=" * 60)

missing_values = df_clean.isnull().sum()
missing_percentage = (df_clean.isnull().sum() / len(df_clean)) * 100

missing_df = pd.DataFrame({
    'Column': df_clean.columns,
    'Missing Count': missing_values.values,
    'Missing %': missing_percentage.values
})

print(missing_df.to_string(index=False))
print(f"\nüìä Total Missing Values: {df_clean.isnull().sum().sum()}")

# Visualize missing values
if df_clean.isnull().sum().sum() > 0:
    plt.figure(figsize=(10, 5))
    sns.heatmap(df_clean.isnull(), cbar=True, yticklabels=False, cmap='viridis')
    plt.title('Missing Values Heatmap', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.show()
else:
    print("\n‚úÖ No missing values found in the dataset!")





# Check current data types
print("=" * 60)
print("CURRENT DATA TYPES")
print("=" * 60)
print(df_clean.dtypes)

# Convert Year to integer (if needed)
df_clean['Year'] = df_clean['Year'].astype(int)

# Convert Country and Sex to category for memory efficiency
df_clean['Country'] = df_clean['Country'].astype('category')
df_clean['Sex'] = df_clean['Sex'].astype('category')

print("\n" + "=" * 60)
print("UPDATED DATA TYPES")
print("=" * 60)
print(df_clean.dtypes)
print("\n‚úÖ Data types verified and optimized!")





# Check for duplicate records
print("=" * 60)
print("DUPLICATE RECORDS ANALYSIS")
print("=" * 60)

duplicates = df_clean.duplicated().sum()
print(f"\nüìä Total Duplicate Rows: {duplicates}")

if duplicates > 0:
    print("\nRemoving duplicate records...")
    df_clean = df_clean.drop_duplicates()
    print(f"‚úÖ Duplicates removed! New shape: {df_clean.shape}")
else:
    print("‚úÖ No duplicate records found!")





# Outlier Detection using IQR Method
print("=" * 60)
print("OUTLIER DETECTION (IQR Method)")
print("=" * 60)

def detect_outliers_iqr(data, column):
    """Detect outliers using the IQR method"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Detect outliers in HALE
outliers, lower, upper = detect_outliers_iqr(df_clean, 'HALE')
print(f"\nüìä HALE Statistics:")
print(f"   - Lower Bound (Q1 - 1.5*IQR): {lower:.2f}")
print(f"   - Upper Bound (Q3 + 1.5*IQR): {upper:.2f}")
print(f"   - Number of Outliers: {len(outliers)}")

# Visualize outliers with boxplot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Box plot
sns.boxplot(data=df_clean, x='Sex', y='HALE', ax=axes[0], palette='Set2')
axes[0].set_title('HALE Distribution by Sex (Boxplot)', fontsize=12, fontweight='bold')
axes[0].set_xlabel('Sex', fontsize=10)
axes[0].set_ylabel('Healthy Life Expectancy (Years)', fontsize=10)

# Histogram with KDE
sns.histplot(data=df_clean, x='HALE', hue='Sex', kde=True, ax=axes[1], palette='Set2')
axes[1].set_title('HALE Distribution (Histogram)', fontsize=12, fontweight='bold')
axes[1].set_xlabel('Healthy Life Expectancy (Years)', fontsize=10)
axes[1].set_ylabel('Frequency', fontsize=10)

plt.tight_layout()
plt.show()

# Note: Not removing outliers as they represent real country data
print("\n‚ö†Ô∏è Note: Outliers represent real variations in life expectancy across countries.")
print("   These will be retained for analysis.")





# Descriptive Statistics
print("=" * 60)
print("DESCRIPTIVE STATISTICS")
print("=" * 60)
df_clean.describe()


# Unique values in categorical columns
print("=" * 60)
print("CATEGORICAL VARIABLES SUMMARY")
print("=" * 60)

print(f"\nüìÖ Years in Dataset: {sorted(df_clean['Year'].unique())}")
print(f"   Total Years: {df_clean['Year'].nunique()}")

print(f"\nüåç Total Countries: {df_clean['Country'].nunique()}")
print(f"\nüë§ Sex Categories: {df_clean['Sex'].unique().tolist()}")


# Final preprocessed dataset summary
print("=" * 60)
print("PREPROCESSING COMPLETE - FINAL DATASET SUMMARY")
print("=" * 60)

print(f"\nüìä Final Dataset Shape: {df_clean.shape}")
print(f"üìã Columns: {df_clean.columns.tolist()}")
print(f"üî¢ Memory Usage: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB")

print("\n" + "=" * 60)
print("SAMPLE OF PREPROCESSED DATA")
print("=" * 60)
df_clean.sample(10)





# Global HALE Trend Analysis
print("=" * 70)
print("GLOBAL HEALTHY LIFE EXPECTANCY TRENDS (2000-2019)")
print("=" * 70)

# Check unique Sex values
print(f"\nüìã Sex categories in data: {df_clean['Sex'].unique().tolist()}")

# Calculate global average HALE by year and sex
global_trend = df_clean.groupby(['Year', 'Sex'])['HALE'].mean().reset_index()

# Pivot for easier plotting
global_pivot = global_trend.pivot(index='Year', columns='Sex', values='HALE')

# Create trend visualization
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Line plot - Trends by Sex
for sex in global_pivot.columns:
    axes[0].plot(global_pivot.index, global_pivot[sex], marker='o', linewidth=2, 
                 markersize=6, label=sex)

axes[0].set_title('Global Average HALE Trend by Sex (2000-2019)', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Year', fontsize=12)
axes[0].set_ylabel('Average Healthy Life Expectancy (Years)', fontsize=12)
axes[0].legend(title='Sex', fontsize=10)
axes[0].grid(True, alpha=0.3)
axes[0].set_xticks(global_pivot.index[::2])

# Calculate year-over-year change - use overall mean if BTSX not available
sex_values = df_clean['Sex'].unique().tolist()
if 'BTSX' in sex_values:
    both_sexes = df_clean[df_clean['Sex'] == 'BTSX'].groupby('Year')['HALE'].mean()
elif 'Both sexes' in sex_values:
    both_sexes = df_clean[df_clean['Sex'] == 'Both sexes'].groupby('Year')['HALE'].mean()
else:
    # Use overall mean across all sexes
    both_sexes = df_clean.groupby('Year')['HALE'].mean()

yoy_change = both_sexes.diff()

# Bar plot - Year-over-Year Change
colors = ['green' if x > 0 else 'red' for x in yoy_change.dropna()]
axes[1].bar(yoy_change.dropna().index, yoy_change.dropna().values, color=colors, alpha=0.7, edgecolor='black')
axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
axes[1].set_title('Year-over-Year Change in Global Average HALE', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Year', fontsize=12)
axes[1].set_ylabel('Change in HALE (Years)', fontsize=12)
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Summary statistics
if len(both_sexes) > 0:
    first_year = both_sexes.index.min()
    last_year = both_sexes.index.max()
    first_val = both_sexes.loc[first_year]
    last_val = both_sexes.loc[last_year]
    years_diff = last_year - first_year
    
    print(f"\nüìà HALE Improvement Summary:")
    print(f"   - {first_year} Average: {first_val:.2f} years")
    print(f"   - {last_year} Average: {last_val:.2f} years")
    print(f"   - Total Improvement: {last_val - first_val:.2f} years")
    print(f"   - Average Annual Increase: {(last_val - first_val)/years_diff:.3f} years")
else:
    print("\n‚ö†Ô∏è No data available for summary statistics")





# Country-wise HALE Analysis (Latest Year)
print("=" * 70)
print("COUNTRY-WISE HALE ANALYSIS")
print("=" * 70)

# Get latest year in dataset
latest_year = df_clean['Year'].max()
print(f"\nüìÖ Latest Year in Data: {latest_year}")

# Check Sex categories and filter appropriately
sex_values = df_clean['Sex'].unique().tolist()
print(f"üìã Sex categories: {sex_values}")

# Filter for latest year - handle different Sex category names
if 'BTSX' in sex_values:
    latest_data = df_clean[(df_clean['Year'] == latest_year) & (df_clean['Sex'] == 'BTSX')]
elif 'Both sexes' in sex_values:
    latest_data = df_clean[(df_clean['Year'] == latest_year) & (df_clean['Sex'] == 'Both sexes')]
else:
    # Use mean across all sexes for each country
    latest_data = df_clean[df_clean['Year'] == latest_year].groupby('Country')['HALE'].mean().reset_index()

print(f"üìä Countries in latest year: {len(latest_data)}")

# Check if we have data
if len(latest_data) > 0:
    # Top 10 and Bottom 10 countries
    top_10 = latest_data.nlargest(10, 'HALE')[['Country', 'HALE']]
    bottom_10 = latest_data.nsmallest(10, 'HALE')[['Country', 'HALE']]

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    # Top 10 Countries
    n_top = len(top_10)
    colors_top = plt.cm.Greens(np.linspace(0.4, 0.9, n_top))
    bars1 = axes[0].barh(top_10['Country'].astype(str), top_10['HALE'], color=colors_top, edgecolor='darkgreen')
    axes[0].set_xlabel('Healthy Life Expectancy (Years)', fontsize=12)
    axes[0].set_title(f'üèÜ Top {n_top} Countries by HALE ({latest_year})', fontsize=14, fontweight='bold')
    axes[0].set_xlim(top_10['HALE'].min() - 5, top_10['HALE'].max() + 3)
    for bar, val in zip(bars1, top_10['HALE']):
        axes[0].text(val + 0.2, bar.get_y() + bar.get_height()/2, f'{val:.1f}', va='center', fontsize=10)

    # Bottom 10 Countries
    n_bottom = len(bottom_10)
    colors_bottom = plt.cm.Reds(np.linspace(0.4, 0.9, n_bottom))[::-1]
    bars2 = axes[1].barh(bottom_10['Country'].astype(str), bottom_10['HALE'], color=colors_bottom, edgecolor='darkred')
    axes[1].set_xlabel('Healthy Life Expectancy (Years)', fontsize=12)
    axes[1].set_title(f'‚ö†Ô∏è Bottom {n_bottom} Countries by HALE ({latest_year})', fontsize=14, fontweight='bold')
    axes[1].set_xlim(bottom_10['HALE'].min() - 5, bottom_10['HALE'].max() + 5)
    for bar, val in zip(bars2, bottom_10['HALE']):
        axes[1].text(val + 0.2, bar.get_y() + bar.get_height()/2, f'{val:.1f}', va='center', fontsize=10)

    plt.tight_layout()
    plt.show()

    # Display disparity
    if len(top_10) > 0 and len(bottom_10) > 0:
        print(f"\nüìä Global HALE Disparity ({latest_year}):")
        print(f"   - Highest HALE: {top_10.iloc[0]['Country']} ({top_10.iloc[0]['HALE']:.2f} years)")
        print(f"   - Lowest HALE: {bottom_10.iloc[0]['Country']} ({bottom_10.iloc[0]['HALE']:.2f} years)")
        print(f"   - Gap: {top_10.iloc[0]['HALE'] - bottom_10.iloc[0]['HALE']:.2f} years")
else:
    print("\n‚ö†Ô∏è No data available for country-wise analysis")





# Gender Gap Analysis
print("=" * 70)
print("GENDER GAP IN HEALTHY LIFE EXPECTANCY")
print("=" * 70)

# Calculate gender gap by country and year
male_data = df_clean[df_clean['Sex'] == 'MLE'].copy()
female_data = df_clean[df_clean['Sex'] == 'FMLE'].copy()

# Merge male and female data
gender_comparison = male_data.merge(
    female_data, 
    on=['Year', 'Country'], 
    suffixes=('_Male', '_Female')
)
gender_comparison['Gender_Gap'] = gender_comparison['HALE_Female'] - gender_comparison['HALE_Male']

# Global average gender gap over time
global_gender_gap = gender_comparison.groupby('Year')['Gender_Gap'].mean()

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Gender gap trend over time
axes[0, 0].plot(global_gender_gap.index, global_gender_gap.values, marker='o', 
                color='purple', linewidth=2, markersize=8)
axes[0, 0].fill_between(global_gender_gap.index, global_gender_gap.values, alpha=0.3, color='purple')
axes[0, 0].set_title('Global Average Gender Gap Over Time', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Year', fontsize=12)
axes[0, 0].set_ylabel('Gender Gap (Female - Male) in Years', fontsize=12)
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)

# 2. Distribution of gender gap (2019)
gap_2019 = gender_comparison[gender_comparison['Year'] == 2019]['Gender_Gap']
axes[0, 1].hist(gap_2019, bins=30, color='mediumpurple', edgecolor='black', alpha=0.7)
axes[0, 1].axvline(gap_2019.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {gap_2019.mean():.2f}')
axes[0, 1].set_title('Distribution of Gender Gap Across Countries (2019)', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Gender Gap (Years)', fontsize=12)
axes[0, 1].set_ylabel('Number of Countries', fontsize=12)
axes[0, 1].legend()

# 3. Male vs Female HALE comparison (2019)
latest_gender = gender_comparison[gender_comparison['Year'] == 2019]
axes[1, 0].scatter(latest_gender['HALE_Male'], latest_gender['HALE_Female'], 
                   alpha=0.6, c='teal', edgecolors='black', s=60)
axes[1, 0].plot([40, 80], [40, 80], 'r--', linewidth=2, label='Equal HALE Line')
axes[1, 0].set_title('Male vs Female HALE by Country (2019)', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Male HALE (Years)', fontsize=12)
axes[1, 0].set_ylabel('Female HALE (Years)', fontsize=12)
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# 4. Countries with largest/smallest gender gaps (2019)
gap_by_country = gender_comparison[gender_comparison['Year'] == 2019][['Country', 'Gender_Gap']].sort_values('Gender_Gap')
top_gap = gap_by_country.tail(10)
bottom_gap = gap_by_country.head(10)
combined = pd.concat([bottom_gap, top_gap])

colors = ['coral' if x < gap_2019.mean() else 'steelblue' for x in combined['Gender_Gap']]
axes[1, 1].barh(combined['Country'].astype(str), combined['Gender_Gap'], color=colors, edgecolor='black')
axes[1, 1].axvline(x=gap_2019.mean(), color='red', linestyle='--', linewidth=2, label=f'Global Mean: {gap_2019.mean():.2f}')
axes[1, 1].set_title('Countries with Extreme Gender Gaps (2019)', fontsize=14, fontweight='bold')
axes[1, 1].set_xlabel('Gender Gap (Female - Male) in Years', fontsize=12)
axes[1, 1].legend()

plt.tight_layout()
plt.show()

print(f"\nüë´ Gender Gap Summary (2019):")
print(f"   - Average Gap: {gap_2019.mean():.2f} years (Females live longer)")
print(f"   - Maximum Gap: {gap_2019.max():.2f} years")
print(f"   - Minimum Gap: {gap_2019.min():.2f} years")





# Distribution Analysis
print("=" * 70)
print("DISTRIBUTION ANALYSIS")
print("=" * 70)

# Check Sex categories
sex_values = df_clean['Sex'].unique().tolist()
print(f"üìã Sex categories: {sex_values}")

# Determine which sex category to use
if 'BTSX' in sex_values:
    sex_filter = 'BTSX'
elif 'Both sexes' in sex_values:
    sex_filter = 'Both sexes'
else:
    sex_filter = None

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. HALE Distribution by Year (Violin Plot)
years_to_plot = [2000, 2005, 2010, 2015, 2019]
available_years = [y for y in years_to_plot if y in df_clean['Year'].unique()]

if sex_filter:
    data_subset = df_clean[(df_clean['Year'].isin(available_years)) & (df_clean['Sex'] == sex_filter)]
else:
    data_subset = df_clean[df_clean['Year'].isin(available_years)]

if len(data_subset) > 0:
    sns.violinplot(data=data_subset, x='Year', y='HALE', ax=axes[0, 0], palette='viridis')
axes[0, 0].set_title('HALE Distribution Evolution', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Year', fontsize=12)
axes[0, 0].set_ylabel('Healthy Life Expectancy (Years)', fontsize=12)

# 2. Confidence Interval Width Analysis
df_clean['CI_Width'] = df_clean['HALE_Upper'] - df_clean['HALE_Lower']
ci_by_year = df_clean.groupby('Year')['CI_Width'].mean()
axes[0, 1].plot(ci_by_year.index, ci_by_year.values, marker='s', color='darkorange', linewidth=2, markersize=8)
axes[0, 1].fill_between(ci_by_year.index, ci_by_year.values, alpha=0.3, color='orange')
axes[0, 1].set_title('Average Confidence Interval Width Over Time', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Year', fontsize=12)
axes[0, 1].set_ylabel('CI Width (Years)', fontsize=12)
axes[0, 1].grid(True, alpha=0.3)

# 3. HALE Distribution Comparison: First vs Last Year
first_year = df_clean['Year'].min()
last_year = df_clean['Year'].max()

if sex_filter:
    hale_first = df_clean[(df_clean['Year'] == first_year) & (df_clean['Sex'] == sex_filter)]['HALE']
    hale_last = df_clean[(df_clean['Year'] == last_year) & (df_clean['Sex'] == sex_filter)]['HALE']
else:
    hale_first = df_clean[df_clean['Year'] == first_year].groupby('Country')['HALE'].mean()
    hale_last = df_clean[df_clean['Year'] == last_year].groupby('Country')['HALE'].mean()

if len(hale_first) > 0:
    axes[1, 0].hist(hale_first, bins=25, alpha=0.6, label=str(first_year), color='coral', edgecolor='black')
    axes[1, 0].axvline(hale_first.mean(), color='coral', linestyle='--', linewidth=2)
if len(hale_last) > 0:
    axes[1, 0].hist(hale_last, bins=25, alpha=0.6, label=str(last_year), color='steelblue', edgecolor='black')
    axes[1, 0].axvline(hale_last.mean(), color='steelblue', linestyle='--', linewidth=2)
axes[1, 0].set_title(f'HALE Distribution: {first_year} vs {last_year}', fontsize=14, fontweight='bold')
axes[1, 0].set_xlabel('Healthy Life Expectancy (Years)', fontsize=12)
axes[1, 0].set_ylabel('Number of Countries', fontsize=12)
axes[1, 0].legend(title='Year')

# 4. Correlation Heatmap
numeric_cols = ['Year', 'HALE', 'HALE_Lower', 'HALE_Upper', 'CI_Width']
corr_matrix = df_clean[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1], 
            fmt='.3f', linewidths=0.5, square=True)
axes[1, 1].set_title('Correlation Matrix', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

# Statistical summary
from scipy import stats
if len(hale_last) > 0:
    print(f"\nüìä Distribution Statistics ({last_year}):")
    print(f"   - Mean HALE: {hale_last.mean():.2f} years")
    print(f"   - Median HALE: {hale_last.median():.2f} years")
    print(f"   - Std Deviation: {hale_last.std():.2f} years")
    print(f"   - Skewness: {stats.skew(hale_last):.3f}")
    print(f"   - Kurtosis: {stats.kurtosis(hale_last):.3f}")





# Country Progress Analysis (First Year to Latest Year)
print("=" * 70)
print("COUNTRY PROGRESS ANALYSIS")
print("=" * 70)

# Get first and last years in dataset
first_year = df_clean['Year'].min()
last_year = df_clean['Year'].max()
print(f"\nüìÖ Analyzing progress from {first_year} to {last_year}")

# Check Sex categories and filter appropriately
sex_values = df_clean['Sex'].unique().tolist()
print(f"üìã Sex categories: {sex_values}")

# Select appropriate sex category or use mean
if 'BTSX' in sex_values:
    analysis_data = df_clean[df_clean['Sex'] == 'BTSX'].copy()
elif 'Both sexes' in sex_values:
    analysis_data = df_clean[df_clean['Sex'] == 'Both sexes'].copy()
else:
    # Use mean across all sexes for each country-year
    analysis_data = df_clean.groupby(['Year', 'Country'])['HALE'].mean().reset_index()

# Calculate improvement for each country
hale_first = analysis_data[analysis_data['Year'] == first_year][['Country', 'HALE']].rename(columns={'HALE': 'HALE_First'})
hale_last = analysis_data[analysis_data['Year'] == last_year][['Country', 'HALE']].rename(columns={'HALE': 'HALE_Last'})

progress_df = hale_first.merge(hale_last, on='Country')
progress_df['Improvement'] = progress_df['HALE_Last'] - progress_df['HALE_First']
progress_df['Improvement_Pct'] = (progress_df['Improvement'] / progress_df['HALE_First']) * 100

print(f"üìä Countries with data for both years: {len(progress_df)}")

# Check if we have data
if len(progress_df) > 0:
    # Sort by improvement
    progress_df = progress_df.sort_values('Improvement', ascending=False)

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    # Top improvers
    n_top = min(15, len(progress_df))
    top_improvers = progress_df.head(n_top)
    colors_improve = plt.cm.Blues(np.linspace(0.4, 0.9, n_top))
    axes[0].barh(top_improvers['Country'].astype(str), top_improvers['Improvement'], 
                 color=colors_improve, edgecolor='darkblue')
    axes[0].set_title(f'üöÄ Top {n_top} Countries by HALE Improvement ({first_year}-{last_year})', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Improvement in HALE (Years)', fontsize=12)
    for i, (idx, row) in enumerate(top_improvers.iterrows()):
        axes[0].text(row['Improvement'] + 0.1, i, f"+{row['Improvement']:.1f}", va='center', fontsize=9)

    # Countries with least improvement or decline
    n_bottom = min(15, len(progress_df))
    least_improvers = progress_df.tail(n_bottom)
    colors_decline = plt.cm.Oranges(np.linspace(0.4, 0.9, n_bottom))[::-1]
    axes[1].barh(least_improvers['Country'].astype(str), least_improvers['Improvement'], 
                 color=colors_decline, edgecolor='darkorange')
    axes[1].set_title(f'‚ö†Ô∏è Countries with Least HALE Improvement ({first_year}-{last_year})', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Change in HALE (Years)', fontsize=12)
    axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)
    for i, (idx, row) in enumerate(least_improvers.iterrows()):
        axes[1].text(row['Improvement'] + 0.1, i, f"{row['Improvement']:+.1f}", va='center', fontsize=9)

    plt.tight_layout()
    plt.show()

    # Get max and min improvement countries safely
    max_country = progress_df.loc[progress_df['Improvement'].idxmax(), 'Country']
    min_country = progress_df.loc[progress_df['Improvement'].idxmin(), 'Country']

    print(f"\nüìà Progress Summary:")
    print(f"   - Countries analyzed: {len(progress_df)}")
    print(f"   - Average Improvement: {progress_df['Improvement'].mean():.2f} years")
    print(f"   - Maximum Improvement: {progress_df['Improvement'].max():.2f} years ({max_country})")
    print(f"   - Minimum Improvement: {progress_df['Improvement'].min():.2f} years ({min_country})")
    print(f"   - Countries with decline: {len(progress_df[progress_df['Improvement'] < 0])}")
else:
    print("\n‚ö†Ô∏è No data available for progress analysis. Check if data exists for both first and last years.")





# Feature Engineering
print("=" * 70)
print("FEATURE ENGINEERING")
print("=" * 70)

from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler

# Create a copy for modeling
df_model = df_clean.copy()

# 4.1 Derived Features
print("\nüìä Creating Derived Features...")

# Time-based features
df_model['Years_Since_2000'] = df_model['Year'] - 2000
df_model['Decade'] = (df_model['Year'] // 10) * 10

# Confidence Interval features
df_model['CI_Width'] = df_model['HALE_Upper'] - df_model['HALE_Lower']
df_model['CI_Relative'] = df_model['CI_Width'] / df_model['HALE'] * 100  # Relative uncertainty

# HALE Categories
df_model['HALE_Category'] = pd.cut(df_model['HALE'], 
                                    bins=[0, 55, 65, 75, 100],
                                    labels=['Low', 'Medium', 'High', 'Very High'])

print("   ‚úÖ Derived features created:")
print("      - Years_Since_2000: Time since baseline year")
print("      - Decade: Decade grouping")
print("      - CI_Width: Confidence interval width")
print("      - CI_Relative: Relative uncertainty (%)")
print("      - HALE_Category: Categorical HALE groupings")


# 4.2 Encoding Categorical Variables
print("\nüìä Encoding Categorical Variables...")

# Label Encoding for Sex
le_sex = LabelEncoder()
df_model['Sex_Encoded'] = le_sex.fit_transform(df_model['Sex'].astype(str))
sex_mapping = dict(zip(le_sex.classes_, le_sex.transform(le_sex.classes_)))
print(f"   ‚úÖ Sex Encoding: {sex_mapping}")

# Label Encoding for Country (for modeling purposes)
le_country = LabelEncoder()
df_model['Country_Encoded'] = le_country.fit_transform(df_model['Country'].astype(str))
print(f"   ‚úÖ Country Encoded: {df_model['Country'].nunique()} unique countries")

# One-Hot Encoding for HALE Category
hale_dummies = pd.get_dummies(df_model['HALE_Category'], prefix='HALE_Cat')
df_model = pd.concat([df_model, hale_dummies], axis=1)
print(f"   ‚úÖ HALE Category One-Hot Encoded: {hale_dummies.columns.tolist()}")


# 4.3 Create Country-Level Aggregated Features
print("\nüìä Creating Country-Level Aggregated Features...")

# Calculate country-level statistics
country_stats = df_model.groupby('Country').agg({
    'HALE': ['mean', 'std', 'min', 'max'],
    'CI_Width': 'mean'
}).reset_index()
country_stats.columns = ['Country', 'Country_HALE_Mean', 'Country_HALE_Std', 
                          'Country_HALE_Min', 'Country_HALE_Max', 'Country_CI_Mean']

# Merge back to main dataframe
df_model = df_model.merge(country_stats, on='Country', how='left')

# Calculate country's relative position
df_model['HALE_vs_Country_Mean'] = df_model['HALE'] - df_model['Country_HALE_Mean']

print("   ‚úÖ Country-level features added:")
print("      - Country_HALE_Mean, Country_HALE_Std")
print("      - Country_HALE_Min, Country_HALE_Max")
print("      - Country_CI_Mean")
print("      - HALE_vs_Country_Mean")


# 4.4 Feature Scaling
print("\nüìä Scaling Numerical Features...")

# Features to scale
features_to_scale = ['Year', 'Years_Since_2000', 'CI_Width', 'CI_Relative', 
                     'Country_HALE_Mean', 'Country_HALE_Std']

# Standard Scaling
scaler_standard = StandardScaler()
scaled_features_standard = scaler_standard.fit_transform(df_model[features_to_scale].fillna(0))
scaled_df_standard = pd.DataFrame(scaled_features_standard, 
                                   columns=[f'{col}_Scaled' for col in features_to_scale])

# MinMax Scaling
scaler_minmax = MinMaxScaler()
scaled_features_minmax = scaler_minmax.fit_transform(df_model[features_to_scale].fillna(0))
scaled_df_minmax = pd.DataFrame(scaled_features_minmax, 
                                 columns=[f'{col}_MinMax' for col in features_to_scale])

# Add scaled features to dataframe
df_model = pd.concat([df_model.reset_index(drop=True), 
                      scaled_df_standard.reset_index(drop=True)], axis=1)

print("   ‚úÖ Standard Scaling applied to numerical features")
print(f"   ‚úÖ Features scaled: {features_to_scale}")


# 4.5 Final Feature Summary
print("\n" + "=" * 70)
print("FEATURE ENGINEERING SUMMARY")
print("=" * 70)

print(f"\nüìä Final Dataset Shape: {df_model.shape}")
print(f"\nüìã All Features ({len(df_model.columns)} columns):")

# Group columns by type
original_cols = ['Year', 'Country', 'Sex', 'HALE', 'HALE_Lower', 'HALE_Upper']
derived_cols = ['Years_Since_2000', 'Decade', 'CI_Width', 'CI_Relative', 'HALE_Category']
encoded_cols = ['Sex_Encoded', 'Country_Encoded'] + [c for c in df_model.columns if 'HALE_Cat_' in c]
aggregated_cols = ['Country_HALE_Mean', 'Country_HALE_Std', 'Country_HALE_Min', 
                   'Country_HALE_Max', 'Country_CI_Mean', 'HALE_vs_Country_Mean']
scaled_cols = [c for c in df_model.columns if '_Scaled' in c]

print(f"\n   Original Features: {original_cols}")
print(f"   Derived Features: {derived_cols}")
print(f"   Encoded Features: {encoded_cols}")
print(f"   Aggregated Features: {aggregated_cols}")
print(f"   Scaled Features: {scaled_cols}")

# Display sample
print("\n" + "=" * 70)
print("SAMPLE OF ENGINEERED DATASET")
print("=" * 70)
df_model.head()





# Import ML Libraries
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import time

print("=" * 70)
print("MODEL DEVELOPMENT - DATA PREPARATION")
print("=" * 70)

# Select features for modeling
feature_cols = ['Years_Since_2000', 'Sex_Encoded', 'Country_Encoded', 
                'Country_HALE_Mean', 'Country_HALE_Std']

# Target variable
target_col = 'HALE'

# Prepare X and y
X = df_model[feature_cols].copy()
y = df_model[target_col].copy()

# Handle any missing values
X = X.fillna(X.mean())

# Train-Test Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nüìä Dataset Split:")
print(f"   - Training Set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"   - Testing Set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"\nüìã Features Used: {feature_cols}")
print(f"üéØ Target Variable: {target_col}")





# Define models to train
print("=" * 70)
print("TRAINING MULTIPLE REGRESSION MODELS")
print("=" * 70)

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),
    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
}

# Store results
results = []

print("\nüîÑ Training models...")
print("-" * 70)

for name, model in models.items():
    start_time = time.time()
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Calculate metrics
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    
    # Cross-validation score
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    training_time = time.time() - start_time
    
    results.append({
        'Model': name,
        'Train_RMSE': train_rmse,
        'Test_RMSE': test_rmse,
        'Train_MAE': train_mae,
        'Test_MAE': test_mae,
        'Train_R2': train_r2,
        'Test_R2': test_r2,
        'CV_R2_Mean': cv_scores.mean(),
        'CV_R2_Std': cv_scores.std(),
        'Training_Time': training_time
    })
    
    print(f"‚úÖ {name:25} | R¬≤ = {test_r2:.4f} | RMSE = {test_rmse:.4f} | Time = {training_time:.2f}s")

print("-" * 70)
print("üéâ All models trained successfully!")





# Hyperparameter Tuning for Random Forest
print("=" * 70)
print("HYPERPARAMETER TUNING - RANDOM FOREST")
print("=" * 70)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

print("\nüîÑ Performing Grid Search (this may take a moment)...")

# Grid Search with Cross-Validation
rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)
grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=0)
grid_search.fit(X_train, y_train)

# Best parameters and model
best_rf = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\n‚úÖ Best Parameters Found:")
for param, value in best_params.items():
    print(f"   - {param}: {value}")

# Evaluate tuned model
y_pred_tuned = best_rf.predict(X_test)
tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))
tuned_mae = mean_absolute_error(y_test, y_pred_tuned)
tuned_r2 = r2_score(y_test, y_pred_tuned)

print(f"\nüìä Tuned Model Performance:")
print(f"   - R¬≤ Score: {tuned_r2:.4f}")
print(f"   - RMSE: {tuned_rmse:.4f}")
print(f"   - MAE: {tuned_mae:.4f}")





# Model Comparison Results
print("=" * 70)
print("MODEL EVALUATION - PERFORMANCE COMPARISON")
print("=" * 70)

# Create results DataFrame
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Test_R2', ascending=False)

# Display formatted results
print("\nüìä Model Performance Comparison (Sorted by Test R¬≤):\n")
display_cols = ['Model', 'Test_R2', 'Test_RMSE', 'Test_MAE', 'CV_R2_Mean', 'CV_R2_Std']
print(results_df[display_cols].to_string(index=False))

# Highlight best model
best_model_name = results_df.iloc[0]['Model']
print(f"\nüèÜ Best Performing Model: {best_model_name}")
print(f"   - Test R¬≤: {results_df.iloc[0]['Test_R2']:.4f}")
print(f"   - Test RMSE: {results_df.iloc[0]['Test_RMSE']:.4f} years")
print(f"   - Test MAE: {results_df.iloc[0]['Test_MAE']:.4f} years")


# Visualization of Model Performance
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. R¬≤ Score Comparison
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(results_df)))
bars = axes[0, 0].barh(results_df['Model'], results_df['Test_R2'], color=colors, edgecolor='black')
axes[0, 0].set_xlabel('R¬≤ Score', fontsize=12)
axes[0, 0].set_title('Model Comparison: R¬≤ Score (Higher is Better)', fontsize=14, fontweight='bold')
axes[0, 0].set_xlim(0, 1)
for bar, val in zip(bars, results_df['Test_R2']):
    axes[0, 0].text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center', fontsize=10)

# 2. RMSE Comparison
colors_rmse = plt.cm.RdYlGn_r(np.linspace(0.3, 0.9, len(results_df)))
bars = axes[0, 1].barh(results_df['Model'], results_df['Test_RMSE'], color=colors_rmse, edgecolor='black')
axes[0, 1].set_xlabel('RMSE (Years)', fontsize=12)
axes[0, 1].set_title('Model Comparison: RMSE (Lower is Better)', fontsize=14, fontweight='bold')
for bar, val in zip(bars, results_df['Test_RMSE']):
    axes[0, 1].text(val + 0.05, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=10)

# 3. Train vs Test R¬≤ (Overfitting Check)
x_pos = np.arange(len(results_df))
width = 0.35
axes[1, 0].bar(x_pos - width/2, results_df['Train_R2'], width, label='Train R¬≤', color='steelblue', edgecolor='black')
axes[1, 0].bar(x_pos + width/2, results_df['Test_R2'], width, label='Test R¬≤', color='coral', edgecolor='black')
axes[1, 0].set_xlabel('Model', fontsize=12)
axes[1, 0].set_ylabel('R¬≤ Score', fontsize=12)
axes[1, 0].set_title('Train vs Test R¬≤ (Overfitting Detection)', fontsize=14, fontweight='bold')
axes[1, 0].set_xticks(x_pos)
axes[1, 0].set_xticklabels(results_df['Model'], rotation=45, ha='right')
axes[1, 0].legend()
axes[1, 0].set_ylim(0, 1.1)

# 4. Cross-Validation Scores with Error Bars
axes[1, 1].errorbar(x_pos, results_df['CV_R2_Mean'], yerr=results_df['CV_R2_Std'], 
                     fmt='o', markersize=10, capsize=5, capthick=2, color='purple', ecolor='gray')
axes[1, 1].set_xlabel('Model', fontsize=12)
axes[1, 1].set_ylabel('CV R¬≤ Score', fontsize=12)
axes[1, 1].set_title('Cross-Validation R¬≤ with Standard Deviation', fontsize=14, fontweight='bold')
axes[1, 1].set_xticks(x_pos)
axes[1, 1].set_xticklabels(results_df['Model'], rotation=45, ha='right')
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()


# Prediction Analysis for Best Model (Tuned Random Forest)
print("=" * 70)
print("PREDICTION ANALYSIS - TUNED RANDOM FOREST")
print("=" * 70)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Actual vs Predicted
axes[0, 0].scatter(y_test, y_pred_tuned, alpha=0.5, color='teal', edgecolors='black', s=50)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual HALE (Years)', fontsize=12)
axes[0, 0].set_ylabel('Predicted HALE (Years)', fontsize=12)
axes[0, 0].set_title('Actual vs Predicted HALE', fontsize=14, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 2. Residual Plot
residuals = y_test - y_pred_tuned
axes[0, 1].scatter(y_pred_tuned, residuals, alpha=0.5, color='purple', edgecolors='black', s=50)
axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0, 1].set_xlabel('Predicted HALE (Years)', fontsize=12)
axes[0, 1].set_ylabel('Residuals (Years)', fontsize=12)
axes[0, 1].set_title('Residual Plot', fontsize=14, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# 3. Residual Distribution
axes[1, 0].hist(residuals, bins=40, color='mediumpurple', edgecolor='black', alpha=0.7)
axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[1, 0].axvline(x=residuals.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean: {residuals.mean():.3f}')
axes[1, 0].set_xlabel('Residuals (Years)', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Distribution of Residuals', fontsize=14, fontweight='bold')
axes[1, 0].legend()

# 4. Feature Importance
feature_importance = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': best_rf.feature_importances_
}).sort_values('Importance', ascending=True)

colors_fi = plt.cm.Blues(np.linspace(0.4, 0.9, len(feature_importance)))
axes[1, 1].barh(feature_importance['Feature'], feature_importance['Importance'], color=colors_fi, edgecolor='darkblue')
axes[1, 1].set_xlabel('Feature Importance', fontsize=12)
axes[1, 1].set_title('Random Forest Feature Importance', fontsize=14, fontweight='bold')
for i, (idx, row) in enumerate(feature_importance.iterrows()):
    axes[1, 1].text(row['Importance'] + 0.01, i, f'{row["Importance"]:.3f}', va='center', fontsize=10)

plt.tight_layout()
plt.show()

print(f"\nüìä Residual Statistics:")
print(f"   - Mean Residual: {residuals.mean():.4f} years")
print(f"   - Std Residual: {residuals.std():.4f} years")
print(f"   - Min Residual: {residuals.min():.4f} years")
print(f"   - Max Residual: {residuals.max():.4f} years")





# Key Insights Summary
print("=" * 80)
print("                    KEY INSIGHTS & CONCLUSIONS")
print("=" * 80)

print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        üìä KEY FINDINGS SUMMARY                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1Ô∏è‚É£  GLOBAL HALE TRENDS (2000-2019)
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚Ä¢ Global average HALE has improved steadily over the past two decades
    ‚Ä¢ Both males and females have experienced improvements
    ‚Ä¢ Females consistently have higher HALE than males globally
    
2Ô∏è‚É£  REGIONAL DISPARITIES
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚Ä¢ Significant gap exists between highest and lowest HALE countries
    ‚Ä¢ Developed nations (Japan, Singapore, Switzerland) lead in HALE
    ‚Ä¢ African nations face the greatest health challenges
    ‚Ä¢ The gap between top and bottom countries exceeds 25+ years
    
3Ô∏è‚É£  GENDER ANALYSIS
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚Ä¢ Women live healthier lives longer than men globally
    ‚Ä¢ Average gender gap: ~3 years in favor of females
    ‚Ä¢ Gender gap varies significantly by country
    
4Ô∏è‚É£  PROGRESS ANALYSIS
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚Ä¢ Most countries have improved their HALE since 2000
    ‚Ä¢ Some countries have made remarkable progress (10+ years improvement)
    ‚Ä¢ A few countries have experienced stagnation or decline
""")

print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ü§ñ MODEL PERFORMANCE SUMMARY                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")

# Best model metrics
print(f"    Best Model: Random Forest (Tuned)")
print(f"    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
print(f"    ‚Ä¢ R¬≤ Score: {tuned_r2:.4f} (explains {tuned_r2*100:.1f}% of variance)")
print(f"    ‚Ä¢ RMSE: {tuned_rmse:.4f} years")
print(f"    ‚Ä¢ MAE: {tuned_mae:.4f} years")

print("""
    
    Key Features (by importance):
    ‚Ä¢ Country historical HALE (mean) - Most predictive
    ‚Ä¢ Time trend (years since 2000)
    ‚Ä¢ Country variability in HALE
    ‚Ä¢ Gender (sex)
""")


# Policy Recommendations and Future Work
print("""
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     üéØ POLICY RECOMMENDATIONS                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. HEALTHCARE INVESTMENT
   ‚Ä¢ Countries with lower HALE should prioritize healthcare infrastructure
   ‚Ä¢ Focus on preventive care and early disease detection
   
2. GENDER-SPECIFIC INTERVENTIONS
   ‚Ä¢ Address factors contributing to lower male life expectancy
   ‚Ä¢ Occupational health, risk behaviors, and mental health support
   
3. REGIONAL COLLABORATION
   ‚Ä¢ Knowledge sharing between high and low HALE countries
   ‚Ä¢ International health programs targeting underperforming regions
   
4. CONTINUOUS MONITORING
   ‚Ä¢ Regular HALE assessments to track progress
   ‚Ä¢ Identify and address emerging health challenges early

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     üîÆ FUTURE IMPROVEMENTS                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. DATA ENRICHMENT
   ‚Ä¢ Incorporate economic indicators (GDP per capita, health expenditure)
   ‚Ä¢ Add education levels and literacy rates
   ‚Ä¢ Include environmental factors (air quality, water access)
   
2. MODEL ENHANCEMENTS
   ‚Ä¢ Implement deep learning models (Neural Networks)
   ‚Ä¢ Time series forecasting (ARIMA, LSTM)
   ‚Ä¢ Ensemble methods combining multiple approaches
   
3. ANALYSIS EXTENSIONS
   ‚Ä¢ Sub-national analysis where data permits
   ‚Ä¢ Age-specific HALE analysis
   ‚Ä¢ Disease-specific impact studies

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           üìù CONCLUSION                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

This analysis demonstrates that:

‚úÖ Healthy Life Expectancy has improved globally over the past two decades
‚úÖ Significant disparities remain between countries and regions
‚úÖ Machine learning models can effectively predict HALE with high accuracy
‚úÖ Historical trends and country characteristics are strong predictors
‚úÖ Targeted interventions can help close the gap between nations

The Random Forest model achieved excellent predictive performance, explaining
over 95% of the variance in HALE values, making it a reliable tool for
health policy planning and forecasting.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                           End of Analysis
                         By: Piyush Ramteke
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
""")


# Final Visualization - Executive Summary Dashboard
fig = plt.figure(figsize=(18, 14))
fig.suptitle('Healthy Life Expectancy Analysis - Executive Summary Dashboard', 
             fontsize=18, fontweight='bold', y=1.02)

# Create grid layout
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)

# Check Sex categories and create trend appropriately
sex_values = df_clean['Sex'].unique().tolist()
if 'BTSX' in sex_values:
    both_sexes_trend = df_clean[df_clean['Sex'] == 'BTSX'].groupby('Year')['HALE'].mean()
elif 'Both sexes' in sex_values:
    both_sexes_trend = df_clean[df_clean['Sex'] == 'Both sexes'].groupby('Year')['HALE'].mean()
else:
    # Use overall mean across all sexes
    both_sexes_trend = df_clean.groupby('Year')['HALE'].mean()

# 1. Global Trend
ax1 = fig.add_subplot(gs[0, 0])
if len(both_sexes_trend) > 0:
    ax1.plot(both_sexes_trend.index, both_sexes_trend.values, marker='o', color='teal', linewidth=2, markersize=6)
    ax1.fill_between(both_sexes_trend.index, both_sexes_trend.values, alpha=0.3, color='teal')
ax1.set_title('Global HALE Trend', fontsize=12, fontweight='bold')
ax1.set_xlabel('Year')
ax1.set_ylabel('Average HALE (Years)')
ax1.grid(True, alpha=0.3)

# 2. Top 5 Countries (Latest Year)
ax2 = fig.add_subplot(gs[0, 1])
# Recreate latest_data if needed
latest_year = df_clean['Year'].max()
if 'BTSX' in sex_values:
    latest_data_dash = df_clean[(df_clean['Year'] == latest_year) & (df_clean['Sex'] == 'BTSX')]
elif 'Both sexes' in sex_values:
    latest_data_dash = df_clean[(df_clean['Year'] == latest_year) & (df_clean['Sex'] == 'Both sexes')]
else:
    latest_data_dash = df_clean[df_clean['Year'] == latest_year].groupby('Country')['HALE'].mean().reset_index()

if len(latest_data_dash) > 0:
    top_5 = latest_data_dash.nlargest(5, 'HALE')
    n_top5 = len(top_5)
    colors_t5 = plt.cm.Greens(np.linspace(0.5, 0.9, n_top5))
    ax2.barh(top_5['Country'].astype(str), top_5['HALE'], color=colors_t5, edgecolor='darkgreen')
    ax2.set_xlim(top_5['HALE'].min() - 5, top_5['HALE'].max() + 2)
ax2.set_title(f'Top 5 Countries ({latest_year})', fontsize=12, fontweight='bold')
ax2.set_xlabel('HALE (Years)')

# 3. Gender Gap Distribution
ax3 = fig.add_subplot(gs[0, 2])
# Check if gap_2019 exists and has data
try:
    if len(gap_2019) > 0:
        ax3.hist(gap_2019, bins=25, color='mediumpurple', edgecolor='black', alpha=0.7)
        ax3.axvline(gap_2019.mean(), color='red', linestyle='--', linewidth=2)
except:
    ax3.text(0.5, 0.5, 'Gender gap data\nnot available', ha='center', va='center', transform=ax3.transAxes)
ax3.set_title('Gender Gap Distribution', fontsize=12, fontweight='bold')
ax3.set_xlabel('Gender Gap (Years)')
ax3.set_ylabel('Countries')

# 4. Model Performance Comparison
ax4 = fig.add_subplot(gs[1, 0])
top_models = results_df.head(4)
colors_m = plt.cm.Blues(np.linspace(0.5, 0.9, len(top_models)))
ax4.barh(top_models['Model'], top_models['Test_R2'], color=colors_m, edgecolor='darkblue')
ax4.set_title('Model R¬≤ Scores', fontsize=12, fontweight='bold')
ax4.set_xlabel('R¬≤ Score')
ax4.set_xlim(min(0.9, top_models['Test_R2'].min() - 0.02), 1.0)

# 5. Actual vs Predicted
ax5 = fig.add_subplot(gs[1, 1])
ax5.scatter(y_test, y_pred_tuned, alpha=0.4, color='teal', s=30)
ax5.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
ax5.set_title('Actual vs Predicted HALE', fontsize=12, fontweight='bold')
ax5.set_xlabel('Actual')
ax5.set_ylabel('Predicted')

# 6. Feature Importance
ax6 = fig.add_subplot(gs[1, 2])
fi_sorted = feature_importance.sort_values('Importance', ascending=True)
colors_fi = plt.cm.Oranges(np.linspace(0.4, 0.9, len(fi_sorted)))
ax6.barh(fi_sorted['Feature'], fi_sorted['Importance'], color=colors_fi, edgecolor='darkorange')
ax6.set_title('Feature Importance', fontsize=12, fontweight='bold')
ax6.set_xlabel('Importance')

# 7-9. Key Metrics Summary
ax7 = fig.add_subplot(gs[2, :])
ax7.axis('off')

# Calculate improvement safely
if len(both_sexes_trend) > 0:
    hale_improvement = both_sexes_trend.iloc[-1] - both_sexes_trend.iloc[0]
else:
    hale_improvement = 0.0

# Get gender gap mean safely
try:
    gender_gap_mean = gap_2019.mean() if len(gap_2019) > 0 else 0.0
except:
    gender_gap_mean = 0.0

# Create summary text box
summary_text = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                              KEY METRICS SUMMARY                                                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üìä Dataset: {len(df_clean):,} observations  |  üåç Countries: {df_clean['Country'].nunique()}  |  üìÖ Period: {df_clean['Year'].min()}-{df_clean['Year'].max()}                            ‚ïë
‚ïë                                                                                                                        ‚ïë
‚ïë  üèÜ Best Model: Random Forest  |  R¬≤ = {tuned_r2:.4f}  |  RMSE = {tuned_rmse:.3f} years  |  MAE = {tuned_mae:.3f} years                   ‚ïë
‚ïë                                                                                                                        ‚ïë
‚ïë  üìà Global HALE Improvement: {hale_improvement:.2f} years  |  üë´ Avg Gender Gap: {gender_gap_mean:.2f} years                                        ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""

ax7.text(0.5, 0.5, summary_text, transform=ax7.transAxes, fontsize=11,
         verticalalignment='center', horizontalalignment='center',
         family='monospace', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))

plt.tight_layout()
plt.show()

print("\n‚úÖ Analysis Complete!")
